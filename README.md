# ğŸ¤– RAG Chat Engine

An intelligent chat engine powered by Retrieval-Augmented Generation (RAG) that enables natural conversations with your documents. Built with Python, it combines LLM technology with vector search to provide accurate, context-aware responses from your knowledge base.

## âœ¨ Features

- **Smart Document Retrieval** - Semantic search using vector embeddings
- **Context-Aware Responses** - Accurate answers grounded in your documents
- **Multi-Format Support** - Process PDFs, text files, and more
- **Conversation Memory** - Maintains chat history for coherent dialogues
- **Flexible Configuration** - Customizable LLM and embedding models

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/Samruddhi-7/RAG-Chat-Engine.git
cd RAG-Chat-Engine

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Configuration

Create a `.env` file in the root directory:

```env
OPENAI_API_KEY=your_api_key_here
EMBEDDING_MODEL=text-embedding-ada-002
LLM_MODEL=gpt-3.5-turbo
CHUNK_SIZE=1000
TOP_K_RESULTS=5
```

### Usage

```bash
cd src
python main.py
```

## ğŸ“ Project Structure

```
RAG-Chat-Engine/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py              # Main application
â”‚   â”œâ”€â”€ chat_engine.py       # Core chat logic
â”‚   â”œâ”€â”€ document_loader.py   # Document processing
â”‚   â”œâ”€â”€ embeddings.py        # Embedding utilities
â”‚   â””â”€â”€ vector_store.py      # Vector database ops
â”œâ”€â”€ data/
â”‚   â””â”€â”€ documents/           # Your documents here
â”œâ”€â”€ .env                     # Environment variables
â””â”€â”€ requirements.txt         # Dependencies
```

## ğŸ’¡ Basic Usage

```python
from src.chat_engine import RAGChatEngine

# Initialize and load documents
chat_engine = RAGChatEngine()
chat_engine.load_documents("data/documents/")

# Chat with your documents
response = chat_engine.chat("What is this document about?")
print(response)
```

## ğŸ› ï¸ Tech Stack

- **Python 3.8+**
- **LangChain / LlamaIndex** - RAG framework
- **OpenAI API** - LLM and embeddings
- **ChromaDB / FAISS** - Vector database
- **PyPDF2** - Document processing

## âš™ï¸ Configuration Options

| Variable | Description | Default |
|----------|-------------|---------|
| `LLM_MODEL` | Language model to use | `gpt-3.5-turbo` |
| `EMBEDDING_MODEL` | Embedding model | `text-embedding-ada-002` |
| `CHUNK_SIZE` | Document chunk size | `1000` |
| `TOP_K_RESULTS` | Retrieved contexts | `5` |
| `TEMPERATURE` | Response creativity | `0.7` |

## ğŸ¯ Use Cases

- Document Q&A systems
- Enterprise knowledge bases
- Research paper analysis
- Customer support automation
- Educational assistants

## ğŸ¤ Contributing

Contributions welcome! Fork the repo, create a feature branch, and submit a pull request.

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

**Samruddhi** - [@Samruddhi-7](https://github.com/Samruddhi-7)

---

â­ Star this repo if you find it helpful!
